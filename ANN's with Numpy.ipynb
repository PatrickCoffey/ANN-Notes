{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some Artificial Neural Networks in python! we will start with some basic implementations in Numpy in this post. After we will move to NumExpr and then once we have the basics i will show you the beautiful Keras library.\n",
    "\n",
    "So first we need a copy of python, and the Numpy Package installed. Numpy is a science computing library for python. basically it gives you some nice algebraic ways to do maths with python, namely the Matrices (ndarray's) and simple functions for dot products etc. Install numpy with ```pip install numpy```.\n",
    "\n",
    "Ok! Let's get started! Take a look at this [post](https://iamtrask.github.io/2015/07/12/basic-python-network/) by iamtrask. this is basically their implementation however i have added the ability to add layers in an easier fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a Nonlinear class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nonlinear(object):\n",
    "    \"\"\"\n",
    "    Nonlinear\n",
    "    ---------\n",
    "    this is used to set up a non linear for a\n",
    "    network. The idea is you can instantiate it \n",
    "    and set what type of non linear function it \n",
    "    will be for that particular neaural network\n",
    "    \"\"\"\n",
    "    \n",
    "    _FUNC_TYPES = ('sigmoid',\n",
    "                   'softmax',\n",
    "                   'relu',\n",
    "                   'tanh',\n",
    "                   'softplus')\n",
    "    \n",
    "    def __init__(self, func_type='sigmoid'):\n",
    "        if func_type in self._FUNC_TYPES:\n",
    "            if func_type == self._FUNC_TYPES[0]:\n",
    "                # sigmoid\n",
    "                self._FUNCTION = self._FUNC_TYPES[0]\n",
    "            elif func_type == self._FUNC_TYPES[1]:\n",
    "                # softmax\n",
    "                self._FUNCTION = self._FUNC_TYPES[1]\n",
    "            elif func_type == self._FUNC_TYPES[2]:\n",
    "                # relu\n",
    "                self._FUNCTION = self._FUNC_TYPES[2]\n",
    "            elif func_type == self._FUNC_TYPES[3]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[3]\n",
    "            elif func_type == self._FUNC_TYPES[4]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[4]\n",
    "        else:\n",
    "            # default to sigmoid on invalid choice?\n",
    "            print(\"incorrect option `{}`\".format(func_type))\n",
    "            print(\"defaulting to sigmoid\")\n",
    "            self._init_sigmoid()\n",
    "    \n",
    "    def __call__(self, x, derivative=False):\n",
    "        ret = None\n",
    "        if self._FUNCTION == self._FUNC_TYPES[0]:\n",
    "            # sigmoid\n",
    "            if derivative:\n",
    "                ret = x*(1-x)\n",
    "            else:\n",
    "                try:\n",
    "                    ret = 1/(1+np.exp(-x))\n",
    "                except:\n",
    "                    ret = 0.0\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[1]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = 2*(np.exp(x) / np.sum(np.exp(x)))\n",
    "            else:\n",
    "                # from: https://gist.github.com/stober/1946926\n",
    "                #e_x = np.exp(x - np.max(x))\n",
    "                #ret = e_x / e_x.sum()\n",
    "                # from: http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "                ret = np.exp(x) / np.sum(np.exp(x))\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[2]:\n",
    "            # relu\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = 2*(abs(x))\n",
    "            else:\n",
    "                ret = x*(abs(x))\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # tanh\n",
    "            if derivative:\n",
    "                # from my own memory of calculus :P\n",
    "                ret = 1.0-x**2\n",
    "            else:\n",
    "                ret = np.tanh(x)\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from wikipedia\n",
    "                ret = 1.0/(1+np.exp(-x))\n",
    "            else:\n",
    "                ret = np.log(1+np.exp(x))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will make it easy to implement a variety of different activation functions!\n",
    "\n",
    "Next, lets implement iamtrask's example as a class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Neural network\n",
    "    --------------\n",
    "    This is my neural netowrk class, it basically holds all \n",
    "    my variables and uses my other functions/classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input, hidden, output, non_lin=Nonlinear(), bias=False, alpha=1, ):\n",
    "        if bias:\n",
    "            self._BIAS = True\n",
    "            self._INPUT = input + 1\n",
    "        else:\n",
    "            self._BIAS = False\n",
    "            self._INPUT = input\n",
    "        self._ALPHA = alpha\n",
    "        self._HIDDEN = hidden\n",
    "        self._OUTPUT = output\n",
    "        self.non_lin = non_lin\n",
    "        self._init_nodes()\n",
    "\n",
    "    def _init_nodes(self):\n",
    "        # set up weights (synapses)\n",
    "        self.w_in = np.random.randn(self._INPUT, self._HIDDEN) \n",
    "        self.w_out = np.random.randn(self._HIDDEN, self._OUTPUT)\n",
    "        # set up changes\n",
    "        #self.change_in = np.zeros((self._INPUT, self._HIDDEN))\n",
    "        #self.change_out = np.zeros((self._HIDDEN, self._OUTPUT))        \n",
    "        \n",
    "    def _do_layer(self, layer_in, weights):\n",
    "        \"\"\"Does the actual calcs between layers :)\"\"\"\n",
    "        ret = self.non_lin(np.dot(layer_in, weights))\n",
    "        return ret\n",
    "    \n",
    "    #def _error_delta(self, layer_in, y):\n",
    "        #layer_error = y - layer_in\n",
    "        #layer_delta = layer_error * self.non_lin(derivative=True)\n",
    "        #return layer_error, layer_delta\n",
    "        \n",
    "    def train(self, x, y, train_loops=1000):\n",
    "        for i in range(train_loops):\n",
    "\n",
    "            # from: https://iamtrask.github.io/2015/07/28/dropout/\n",
    "            \n",
    "            # Why Dropout: Dropout helps prevent weights from converging to \n",
    "            # identical positions. It does this by randomly turning nodes off \n",
    "            # when forward propagating. It then back-propagates with all the \n",
    "            # nodes turned on.\n",
    "            # A good initial configuration for this for hidden layers is 50%. \n",
    "            # If applying dropout to an input layer, it's best to not exceed \n",
    "            # 25%.\n",
    "            # use Dropout during training. Do not use it at runtime or on your \n",
    "            # testing dataset.\n",
    "            \n",
    "            #if do_dropout:\n",
    "                #layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],\n",
    "                                          #1-dropout_percent)[0] * \\\n",
    "                                          #(1.0/(1-dropout_percent))\n",
    "            \n",
    "            # set up layers\n",
    "            layer0 = x\n",
    "            layer1 = self._do_layer(layer0, \n",
    "                                    self.w_in)\n",
    "            layer2 = self._do_layer(layer1,\n",
    "                                    self.w_out)\n",
    "            \n",
    "            # calculate errors\n",
    "            layer2_error = y - layer2\n",
    "            layer2_delta = layer2_error * self.non_lin(layer2, derivative=True)\n",
    "            \n",
    "            layer1_error = layer2_delta.dot(self.w_out.T)\n",
    "            layer1_delta = layer1_error * self.non_lin(layer1, derivative=True)\n",
    "            \n",
    "            if (i % (train_loops/10)) == 0:\n",
    "                print(\"loop: {}\".format(i))\n",
    "                print(\"Layer1 Error: {}\".format(np.mean(np.abs(layer1_error))))                \n",
    "                print(\"Layer2 Error: {}\".format(np.mean(np.abs(layer2_error))))\n",
    "                print(\"Guess: \")\n",
    "                print(layer2[0])               \n",
    "                print(\"Guess (round): \")\n",
    "                print(np.round(layer2[0], 1))\n",
    "                print(\"Actual: \")\n",
    "                print(y[0])\n",
    "                \n",
    "            #if (i % (train_loops/100)) == 0:\n",
    "                #print(\"currently on loop: {}\".format(i))\n",
    "            # backpropagate error\n",
    "            self.w_out += self._ALPHA * layer1.T.dot(layer2_delta)\n",
    "            self.w_in += self._ALPHA * layer0.T.dot(layer1_delta)\n",
    "            \n",
    "    \n",
    "    def guess(self, x):\n",
    "        _in = x\n",
    "        _hidden = self.non_lin(np.dot(_in, self.w_in))\n",
    "        _out = self.non_lin(np.dot(_hidden, self.w_out))\n",
    "        return _out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how its basically the exact same code but wrapped up in a class? This makes it easy to import and pass around inside of an application!\n",
    "\n",
    "Now, lets look at implementing a multi layer version of this network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNN(object):\n",
    "    \"\"\"N-layered neural network\"\"\"\n",
    "    def __init__(self, inputs, weights, outputs, alpha):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self._ALPHA = alpha\n",
    "        self._num_of_weights = len(weights)\n",
    "        self._LAYER_DEFS = {}\n",
    "        self.WEIGHT_DATA = {}\n",
    "        self.LAYER_FUNC = {}\n",
    "        self.LAYERS = {}\n",
    "        for i in range(self._num_of_weights):\n",
    "            #(in, out, nonlin)\n",
    "            self._LAYER_DEFS[i] = {'in': weights[i][0],\n",
    "                              'out': weights[i][1],\n",
    "                              'nonlin': weights[i][2]}\n",
    "        print(self._LAYER_DEFS)\n",
    "        self._init_layers()\n",
    "    \n",
    "    def _init_layers(self):\n",
    "        for i in range(self._num_of_weights):\n",
    "            _in = self._LAYER_DEFS[i]['in']\n",
    "            _out = self._LAYER_DEFS[i]['out']\n",
    "            _nonlin = self._LAYER_DEFS[i]['nonlin']\n",
    "            self.WEIGHT_DATA[i] = np.random.randn(_in, _out)\n",
    "            self.LAYER_FUNC[i] = _nonlin\n",
    "    \n",
    "    def _do_layer(self, prev_layer, next_layer, nonlin):\n",
    "        \"\"\"Does the actual calcs between layers :)\"\"\"\n",
    "        ret = nonlin(np.dot(prev_layer, next_layer))\n",
    "        return ret\n",
    "\n",
    "    def train(self, x, y, train_loops=100):\n",
    "        for j in range(train_loops):\n",
    "            # set up layers\n",
    "            prev_layer = x\n",
    "            prev_y = y\n",
    "            next_weight = None\n",
    "            l = 0\n",
    "            self.LAYERS[l] = x\n",
    "            for i in range(self._num_of_weights):\n",
    "                l += 1\n",
    "                next_weight = self.WEIGHT_DATA[i]\n",
    "                nonlin = self.LAYER_FUNC[i]\n",
    "                current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "                self.LAYERS[l] = current_layer\n",
    "                prev_layer = current_layer\n",
    "            last_layer = current_layer\n",
    "            #print(last_layer)\n",
    "            #\n",
    "            #layer2_error = y - layer2\n",
    "            #layer2_delta = layer2_error * self.non_lin(layer2, derivative=True)\n",
    "            \n",
    "            #layer1_error = layer2_delta.dot(self.w_out.T)\n",
    "            #layer1_delta = layer1_error * self.non_lin(layer1, derivative=True)\n",
    "            \n",
    "            #self.w_out += self._ALPHA * layer1.T.dot(layer2_delta)\n",
    "            #self.w_in += self._ALPHA * layer0.T.dot(layer1_delta)              \n",
    "            \n",
    "            # calculate errors\n",
    "            output_error = y - last_layer\n",
    "            output_nonlin = self.LAYER_FUNC[self._num_of_weights - 1]\n",
    "            output_delta = output_error * output_nonlin(last_layer, derivative=True)\n",
    "\n",
    "            prev_delta = output_delta\n",
    "            prev_layer = last_layer\n",
    "            for i in reversed(range(self._num_of_weights)):\n",
    "                weight = self.WEIGHT_DATA[i]\n",
    "                current_weight_error = prev_delta.dot(weight.T)\n",
    "                current_weight_nonlin = self.LAYER_FUNC[i]\n",
    "                current_weight_delta = current_weight_error * current_weight_nonlin(self.LAYERS[i], derivative=True)\n",
    "                # backpropagate error\n",
    "                self.WEIGHT_DATA[i] += self._ALPHA * self.LAYERS[i].T.dot(prev_delta)\n",
    "                prev_delta = current_weight_delta\n",
    "                \n",
    "\n",
    "            if (j % (train_loops/10)) == 0:\n",
    "                print(\"loop: {}\".format(j))\n",
    "                #print(\"Layer1 Error: {}\".format(np.mean(np.abs(layer1_error))))                \n",
    "                #print(\"Layer2 Error: {}\".format(np.mean(np.abs(layer2_error))))\n",
    "                #print(\"Guess: \")\n",
    "                #print(last_layer[0])\n",
    "                #print(\"output delta: \")\n",
    "                #print(np.round(output_delta, 2))\n",
    "                print(\"Guess (rounded): \")\n",
    "                print(np.round(last_layer[0], 1))\n",
    "                print(\"Actual: \")\n",
    "                print(y[0])\n",
    "        \n",
    "    def guess(self, x):\n",
    "        prev_layer = x\n",
    "        prev_y = y\n",
    "        next_weight = None\n",
    "        l = 0\n",
    "        self.LAYERS[l] = x\n",
    "        for i in range(self._num_of_weights):\n",
    "            l += 1\n",
    "            next_weight = self.WEIGHT_DATA[i]\n",
    "            nonlin = self.LAYER_FUNC[i]\n",
    "            current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "            self.LAYERS[l] = current_layer\n",
    "            prev_layer = current_layer\n",
    "        last_layer = current_layer\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how the training function and the guess function loop through the layers? They pass them to an internal helper function ```_do_layer```.\n",
    "\n",
    "Now, to make it easy to load the mnist! we will be using an external script from [here](https://gist.github.com/akesling/5358964)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "# from: https://gist.github.com/akesling/5358964\n",
    "\n",
    "\"\"\"\n",
    "Loosely inspired by http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "which is GPL licensed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read(dataset=\"training\", path=\"./data\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set. It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise(ValueError, \"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "\n",
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "def get_flat_mnist(dataset=\"training\", path=\"./mnist\", items=60000, normalize=False):\n",
    "    images = tuple()\n",
    "    labels = tuple()\n",
    "    i = 0\n",
    "    for image in read(dataset, path):\n",
    "        images += (image[1],)\n",
    "        labels += (image[0],)\n",
    "        i += 1\n",
    "        if i == items:\n",
    "            break\n",
    "\n",
    "    flat_images = tuple()\n",
    "    for image in images:\n",
    "        flat_image = []\n",
    "        for row in image:\n",
    "            l_row = list(row)\n",
    "            for item in l_row:\n",
    "                if normalize:\n",
    "                    if item <= 127:\n",
    "                        flat_image.append(0)\n",
    "                    else:\n",
    "                        flat_image.append(1)\n",
    "                else:\n",
    "                    flat_image.append(item)\n",
    "        flat_images += (flat_image,)\n",
    "    del images\n",
    "\n",
    "\n",
    "    out_labels = tuple()\n",
    "    # [0,1,2,3,4,5,6,7,8,9]\n",
    "    for item in labels:\n",
    "        if item == 0:\n",
    "            out_labels += ([1,0,0,0,0,0,0,0,0,0],)\n",
    "        elif item == 1:\n",
    "            out_labels += ([0,1,0,0,0,0,0,0,0,0],)\n",
    "        elif item == 2:\n",
    "            out_labels += ([0,0,1,0,0,0,0,0,0,0],)\n",
    "        elif item == 3:\n",
    "            out_labels += ([0,0,0,1,0,0,0,0,0,0],)\n",
    "        elif item == 4:\n",
    "            out_labels += ([0,0,0,0,1,0,0,0,0,0],)\n",
    "        elif item == 5:\n",
    "            out_labels += ([0,0,0,0,0,1,0,0,0,0],)\n",
    "        elif item == 6:\n",
    "            out_labels += ([0,0,0,0,0,0,1,0,0,0],)\n",
    "        elif item == 7:\n",
    "            out_labels += ([0,0,0,0,0,0,0,1,0,0],)\n",
    "        elif item == 8:\n",
    "            out_labels += ([0,0,0,0,0,0,0,0,1,0],)\n",
    "        elif item == 9:\n",
    "            out_labels += ([0,0,0,0,0,0,0,0,0,1],)\n",
    "    return flat_images, out_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added a function ```get_flat_mnist``` to get a flattened version of the mnist dataset. This basically means that each image of the data has been appended onto the end of the first row in order to produce and 784 item long single row array. The answer row has been adjusted to fit the format that the output neurons will fire. You can see that the positions in the array represent the numbers 0 to 9. The position that has a 1 at it represents the number the image corresponded to!\n",
    "\n",
    "Now lets quickly run another script from the same folder we are working in. This will load some different sized data sets and pickle them with cpickle (this requires cpickle, or atleast pickle installed, you can do this with ```pip install cpickle```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "def make_mnist_np():\n",
    "    l_items = [100, 500, 1000, 2000, 5000, 10000, 20000]\n",
    "    for items in l_items:\n",
    "        print(\"grabbing {} data...\".format(items))\n",
    "        t_in, t_out = mnist.get_flat_mnist(items=items, normalize=True)\n",
    "        print(\"  got nmist array!\")\n",
    "        print('  {}x{}'.format(len(t_in), len(t_in[0])))\n",
    "        x = np.array(t_in, dtype=np.float)\n",
    "        y = np.array(t_out, dtype=np.float)\n",
    "        with open('mnist/tx{}'.format(items), 'wb+') as f:\n",
    "            pickle.dump(x, f)\n",
    "        with open('mnist/ty{}'.format(items), 'wb+') as f:\n",
    "            pickle.dump(y, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_mnist_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make sense when we implement the mini menu system in a second. Back in the original file we defined the neural network and the nonlinear function, lets start to use some of the things we have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import mnist\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except:\n",
    "        import pickle\n",
    "\n",
    "\n",
    "    # get data\n",
    "    if input(\"load mnist training data?\").lower() == 'y':\n",
    "        load_d = input(\"  enter filename (eg. 500 = tx-500, ty-500): \")\n",
    "        with open(\"mnist/tx{}\".format(load_d), 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "        with open(\"mnist/ty{}\".format(load_d), 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "    else:\n",
    "        print(\"grabbing data...\")\n",
    "        t_in, t_out = mnist.get_flat_mnist(items=1000, normalize=True)\n",
    "        print(\"  got nmist array!\")\n",
    "        print('  {}x{}'.format(len(t_in), len(t_in[0])))\n",
    "        x = np.array(t_in, dtype=np.float)\n",
    "        y = np.array(t_out, dtype=np.float)        \n",
    "    \n",
    "    \n",
    "    load = input(\"load network? (y/N): \")\n",
    "    if load.lower() == 'y':\n",
    "        fname = input(\"network filename: \")\n",
    "        with open(fname, 'rb') as f:\n",
    "            nnn = pickle.load(f)\n",
    "    else:\n",
    "        # set hypervariables\n",
    "        i_input = 784 # this is how many pixel per image (they are flat)\n",
    "        i_out = 10\n",
    "    \n",
    "        # 4 hidden layer network!\n",
    "        weights = ((784, 512, NENonlinear('sigmoid')), \n",
    "                   (512, 256, NENonlinear('sigmoid')),\n",
    "                   (256, 16, NENonlinear('sigmoid')),\n",
    "                   (16, 10, NENonlinear('sigmoid')))\n",
    "    \n",
    "        # initialise network\n",
    "        print(\"initialising network...\")\n",
    "        #nn = NeuralNetwork(i_input, i_hidden, i_out, Nonlinear('sigmoid'), False, 0.1)\n",
    "        nnn = NENNN(inputs=i_input, \n",
    "                    weights=weights, \n",
    "                    outputs=i_out,\n",
    "                    alpha=0.01)\n",
    "        print(\"  network initialised!\")\n",
    "    \n",
    "    # train networkn\n",
    "    loops = 100\n",
    "    print(\"training network for {} loops\".format(loops))\n",
    "    nnn.train(x, y, loops)\n",
    "    \n",
    "    save = input(\"save network? (y/N): \")\n",
    "    if save.lower() == 'y':\n",
    "        fname = input(\"save network as: \")\n",
    "        with open(fname, 'wb+') as f:\n",
    "            pickle.dump(nnn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the file in full!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Schlerp's neural network\n",
    "# ----------------------\n",
    "#\n",
    "# a bunch of shit for me to fuck around with and learn \n",
    "# neural networks.\n",
    "# currently idea is:\n",
    "#  * pick a non linear\n",
    "#  \n",
    "#  * initialise a network with the amount of input,\n",
    "#    hidden, and output nodes that you want for the \n",
    "#    data\n",
    "#  \n",
    "#  * train the network with training x and y\n",
    "#  \n",
    "#  * test it using nn.guess(x) with a known y\n",
    "#  \n",
    "#  * if happy, test with real world data :P\n",
    "#\n",
    "\n",
    "\n",
    "# example 6 layer nn for solving mnist\n",
    "# i  -h   -h   -h   -h   -h  -o\n",
    "# 784-2500-2000-1500-1000-500-10\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Nonlinear(object):\n",
    "    \"\"\"\n",
    "    Nonlinear\n",
    "    ---------\n",
    "    this is used to set up a non linear for a\n",
    "    network. The idea is you can instantiate it \n",
    "    and set what type of non linear function it \n",
    "    will be for that particular neaural network\n",
    "    \"\"\"\n",
    "    \n",
    "    _FUNC_TYPES = ('sigmoid',\n",
    "                   'softmax',\n",
    "                   'relu',\n",
    "                   'tanh',\n",
    "                   'softplus')\n",
    "    \n",
    "    def __init__(self, func_type='sigmoid'):\n",
    "        if func_type in self._FUNC_TYPES:\n",
    "            if func_type == self._FUNC_TYPES[0]:\n",
    "                # sigmoid\n",
    "                self._FUNCTION = self._FUNC_TYPES[0]\n",
    "            elif func_type == self._FUNC_TYPES[1]:\n",
    "                # softmax\n",
    "                self._FUNCTION = self._FUNC_TYPES[1]\n",
    "            elif func_type == self._FUNC_TYPES[2]:\n",
    "                # relu\n",
    "                self._FUNCTION = self._FUNC_TYPES[2]\n",
    "            elif func_type == self._FUNC_TYPES[3]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[3]\n",
    "            elif func_type == self._FUNC_TYPES[4]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[4]\n",
    "        else:\n",
    "            # default to sigmoid on invalid choice?\n",
    "            print(\"incorrect option `{}`\".format(func_type))\n",
    "            print(\"defaulting to sigmoid\")\n",
    "            self._init_sigmoid()\n",
    "    \n",
    "    def __call__(self, x, derivative=False):\n",
    "        ret = None\n",
    "        if self._FUNCTION == self._FUNC_TYPES[0]:\n",
    "            # sigmoid\n",
    "            if derivative:\n",
    "                ret = x*(1-x)\n",
    "            else:\n",
    "                try:\n",
    "                    ret = 1/(1+np.exp(-x))\n",
    "                except:\n",
    "                    ret = 0.0\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[1]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = 2*(np.exp(x) / np.sum(np.exp(x)))\n",
    "            else:\n",
    "                # from: https://gist.github.com/stober/1946926\n",
    "                #e_x = np.exp(x - np.max(x))\n",
    "                #ret = e_x / e_x.sum()\n",
    "                # from: http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "                ret = np.exp(x) / np.sum(np.exp(x))\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[2]:\n",
    "            # relu\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = 2*(abs(x))\n",
    "            else:\n",
    "                ret = x*(abs(x))\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # tanh\n",
    "            if derivative:\n",
    "                # from my own memory of calculus :P\n",
    "                ret = 1.0-x**2\n",
    "            else:\n",
    "                ret = np.tanh(x)\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from wikipedia\n",
    "                ret = 1.0/(1+np.exp(-x))\n",
    "            else:\n",
    "                ret = np.log(1+np.exp(x))\n",
    "        return ret\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Neural network\n",
    "    --------------\n",
    "    This is my neural netowrk class, it basically holds all \n",
    "    my variables and uses my other functions/classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input, hidden, output, non_lin=Nonlinear(), bias=False, alpha=1, ):\n",
    "        if bias:\n",
    "            self._BIAS = True\n",
    "            self._INPUT = input + 1\n",
    "        else:\n",
    "            self._BIAS = False\n",
    "            self._INPUT = input\n",
    "        self._ALPHA = alpha\n",
    "        self._HIDDEN = hidden\n",
    "        self._OUTPUT = output\n",
    "        self.non_lin = non_lin\n",
    "        self._init_nodes()\n",
    "\n",
    "    def _init_nodes(self):\n",
    "        # set up weights (synapses)\n",
    "        self.w_in = np.random.randn(self._INPUT, self._HIDDEN) \n",
    "        self.w_out = np.random.randn(self._HIDDEN, self._OUTPUT)\n",
    "        # set up changes\n",
    "        #self.change_in = np.zeros((self._INPUT, self._HIDDEN))\n",
    "        #self.change_out = np.zeros((self._HIDDEN, self._OUTPUT))        \n",
    "        \n",
    "    def _do_layer(self, layer_in, weights):\n",
    "        \"\"\"Does the actual calcs between layers :)\"\"\"\n",
    "        ret = self.non_lin(np.dot(layer_in, weights))\n",
    "        return ret\n",
    "    \n",
    "    #def _error_delta(self, layer_in, y):\n",
    "        #layer_error = y - layer_in\n",
    "        #layer_delta = layer_error * self.non_lin(derivative=True)\n",
    "        #return layer_error, layer_delta\n",
    "        \n",
    "    def train(self, x, y, train_loops=1000):\n",
    "        for i in range(train_loops):\n",
    "\n",
    "            # from: https://iamtrask.github.io/2015/07/28/dropout/\n",
    "            \n",
    "            # Why Dropout: Dropout helps prevent weights from converging to \n",
    "            # identical positions. It does this by randomly turning nodes off \n",
    "            # when forward propagating. It then back-propagates with all the \n",
    "            # nodes turned on.\n",
    "            # A good initial configuration for this for hidden layers is 50%. \n",
    "            # If applying dropout to an input layer, it's best to not exceed \n",
    "            # 25%.\n",
    "            # use Dropout during training. Do not use it at runtime or on your \n",
    "            # testing dataset.\n",
    "            \n",
    "            #if do_dropout:\n",
    "                #layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],\n",
    "                                          #1-dropout_percent)[0] * \\\n",
    "                                          #(1.0/(1-dropout_percent))\n",
    "            \n",
    "            # set up layers\n",
    "            layer0 = x\n",
    "            layer1 = self._do_layer(layer0, \n",
    "                                    self.w_in)\n",
    "            layer2 = self._do_layer(layer1,\n",
    "                                    self.w_out)\n",
    "            \n",
    "            # calculate errors\n",
    "            layer2_error = y - layer2\n",
    "            layer2_delta = layer2_error * self.non_lin(layer2, derivative=True)\n",
    "            \n",
    "            layer1_error = layer2_delta.dot(self.w_out.T)\n",
    "            layer1_delta = layer1_error * self.non_lin(layer1, derivative=True)\n",
    "            \n",
    "            if (i % (train_loops/10)) == 0:\n",
    "                print(\"loop: {}\".format(i))\n",
    "                print(\"Layer1 Error: {}\".format(np.mean(np.abs(layer1_error))))                \n",
    "                print(\"Layer2 Error: {}\".format(np.mean(np.abs(layer2_error))))\n",
    "                print(\"Guess: \")\n",
    "                print(layer2[0])               \n",
    "                print(\"Guess (round): \")\n",
    "                print(np.round(layer2[0], 1))\n",
    "                print(\"Actual: \")\n",
    "                print(y[0])\n",
    "                \n",
    "            #if (i % (train_loops/100)) == 0:\n",
    "                #print(\"currently on loop: {}\".format(i))\n",
    "            # backpropagate error\n",
    "            self.w_out += self._ALPHA * layer1.T.dot(layer2_delta)\n",
    "            self.w_in += self._ALPHA * layer0.T.dot(layer1_delta)\n",
    "            \n",
    "    \n",
    "    def guess(self, x):\n",
    "        _in = x\n",
    "        _hidden = self.non_lin(np.dot(_in, self.w_in))\n",
    "        _out = self.non_lin(np.dot(_hidden, self.w_out))\n",
    "        return _out\n",
    "\n",
    "\n",
    "class NNN(object):\n",
    "    \"\"\"N-layered neural network\"\"\"\n",
    "    def __init__(self, inputs, weights, outputs, alpha):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self._ALPHA = alpha\n",
    "        self._num_of_weights = len(weights)\n",
    "        self._LAYER_DEFS = {}\n",
    "        self.WEIGHT_DATA = {}\n",
    "        self.LAYER_FUNC = {}\n",
    "        self.LAYERS = {}\n",
    "        for i in range(self._num_of_weights):\n",
    "            #(in, out, nonlin)\n",
    "            self._LAYER_DEFS[i] = {'in': weights[i][0],\n",
    "                              'out': weights[i][1],\n",
    "                              'nonlin': weights[i][2]}\n",
    "        print(self._LAYER_DEFS)\n",
    "        self._init_layers()\n",
    "    \n",
    "    def _init_layers(self):\n",
    "        for i in range(self._num_of_weights):\n",
    "            _in = self._LAYER_DEFS[i]['in']\n",
    "            _out = self._LAYER_DEFS[i]['out']\n",
    "            _nonlin = self._LAYER_DEFS[i]['nonlin']\n",
    "            self.WEIGHT_DATA[i] = np.random.randn(_in, _out)\n",
    "            self.LAYER_FUNC[i] = _nonlin\n",
    "    \n",
    "    def _do_layer(self, prev_layer, next_layer, nonlin):\n",
    "        \"\"\"Does the actual calcs between layers :)\"\"\"\n",
    "        ret = nonlin(np.dot(prev_layer, next_layer))\n",
    "        return ret\n",
    "\n",
    "    def train(self, x, y, train_loops=100):\n",
    "        for j in range(train_loops):\n",
    "            # set up layers\n",
    "            prev_layer = x\n",
    "            prev_y = y\n",
    "            next_weight = None\n",
    "            l = 0\n",
    "            self.LAYERS[l] = x\n",
    "            for i in range(self._num_of_weights):\n",
    "                l += 1\n",
    "                next_weight = self.WEIGHT_DATA[i]\n",
    "                nonlin = self.LAYER_FUNC[i]\n",
    "                current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "                self.LAYERS[l] = current_layer\n",
    "                prev_layer = current_layer\n",
    "            last_layer = current_layer\n",
    "            #print(last_layer)\n",
    "            #\n",
    "            #layer2_error = y - layer2\n",
    "            #layer2_delta = layer2_error * self.non_lin(layer2, derivative=True)\n",
    "            \n",
    "            #layer1_error = layer2_delta.dot(self.w_out.T)\n",
    "            #layer1_delta = layer1_error * self.non_lin(layer1, derivative=True)\n",
    "            \n",
    "            #self.w_out += self._ALPHA * layer1.T.dot(layer2_delta)\n",
    "            #self.w_in += self._ALPHA * layer0.T.dot(layer1_delta)              \n",
    "            \n",
    "            # calculate errors\n",
    "            output_error = y - last_layer\n",
    "            output_nonlin = self.LAYER_FUNC[self._num_of_weights - 1]\n",
    "            output_delta = output_error * output_nonlin(last_layer, derivative=True)\n",
    "\n",
    "            prev_delta = output_delta\n",
    "            prev_layer = last_layer\n",
    "            for i in reversed(range(self._num_of_weights)):\n",
    "                weight = self.WEIGHT_DATA[i]\n",
    "                current_weight_error = prev_delta.dot(weight.T)\n",
    "                current_weight_nonlin = self.LAYER_FUNC[i]\n",
    "                current_weight_delta = current_weight_error * current_weight_nonlin(self.LAYERS[i], derivative=True)\n",
    "                # backpropagate error\n",
    "                self.WEIGHT_DATA[i] += self._ALPHA * self.LAYERS[i].T.dot(prev_delta)\n",
    "                prev_delta = current_weight_delta\n",
    "                \n",
    "\n",
    "            if (j % (train_loops/10)) == 0:\n",
    "                print(\"loop: {}\".format(j))\n",
    "                #print(\"Layer1 Error: {}\".format(np.mean(np.abs(layer1_error))))                \n",
    "                #print(\"Layer2 Error: {}\".format(np.mean(np.abs(layer2_error))))\n",
    "                #print(\"Guess: \")\n",
    "                #print(last_layer[0])\n",
    "                #print(\"output delta: \")\n",
    "                #print(np.round(output_delta, 2))\n",
    "                print(\"Guess (rounded): \")\n",
    "                print(np.round(last_layer[0], 1))\n",
    "                print(\"Actual: \")\n",
    "                print(y[0])\n",
    "        \n",
    "    def guess(self, x):\n",
    "        prev_layer = x\n",
    "        prev_y = y\n",
    "        next_weight = None\n",
    "        l = 0\n",
    "        self.LAYERS[l] = x\n",
    "        for i in range(self._num_of_weights):\n",
    "            l += 1\n",
    "            next_weight = self.WEIGHT_DATA[i]\n",
    "            nonlin = self.LAYER_FUNC[i]\n",
    "            current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "            self.LAYERS[l] = current_layer\n",
    "            prev_layer = current_layer\n",
    "        last_layer = current_layer\n",
    "        return last_layer\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    import mnist\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except:\n",
    "        import pickle\n",
    "\n",
    "\n",
    "    # get data\n",
    "    if input(\"load mnist training data?\").lower() == 'y':\n",
    "        load_d = input(\"  enter filename (eg. 500 = tx-500, ty-500): \")\n",
    "        with open(\"mnist/tx{}\".format(load_d), 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "        with open(\"mnist/ty{}\".format(load_d), 'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "    else:\n",
    "        print(\"grabbing data...\")\n",
    "        t_in, t_out = mnist.get_flat_mnist(items=1000, normalize=True)\n",
    "        print(\"  got nmist array!\")\n",
    "        print('  {}x{}'.format(len(t_in), len(t_in[0])))\n",
    "        x = np.array(t_in, dtype=np.float)\n",
    "        y = np.array(t_out, dtype=np.float)        \n",
    "    \n",
    "    \n",
    "    load = input(\"load network? (y/N): \")\n",
    "    if load.lower() == 'y':\n",
    "        fname = input(\"network filename: \")\n",
    "        with open(fname, 'rb') as f:\n",
    "            nnn = pickle.load(f)\n",
    "    else:\n",
    "        # set hypervariables\n",
    "        i_input = 784 # this is how many pixel per image (they are flat)\n",
    "        i_out = 10\n",
    "    \n",
    "        # 4 hidden layer network!\n",
    "        weights = ((784, 512, NENonlinear('sigmoid')), \n",
    "                   (512, 256, NENonlinear('sigmoid')),\n",
    "                   (256, 16, NENonlinear('sigmoid')),\n",
    "                   (16, 10, NENonlinear('sigmoid')))\n",
    "    \n",
    "        \n",
    "        # initialise network\n",
    "        print(\"initialising network...\")\n",
    "        #nn = NeuralNetwork(i_input, i_hidden, i_out, Nonlinear('sigmoid'), False, 0.1)\n",
    "        nnn = NENNN(inputs=i_input, \n",
    "                    weights=weights, \n",
    "                    outputs=i_out,\n",
    "                    alpha=0.01)\n",
    "        print(\"  network initialised!\")\n",
    "    \n",
    "    # train networkn\n",
    "    loops = 100\n",
    "    print(\"training network for {} loops\".format(loops))\n",
    "    nnn.train(x, y, loops)\n",
    "    \n",
    "    save = input(\"save network? (y/N): \")\n",
    "    if save.lower() == 'y':\n",
    "        fname = input(\"save network as: \")\n",
    "        with open(fname, 'wb+') as f:\n",
    "            pickle.dump(nnn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a play and tell me what you think!\n",
    "\n",
    "(also, look for the second post when we implement this using numexpr, a C library that can speed up some of these operations quite a bit!)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
