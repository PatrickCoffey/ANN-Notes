{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement an Artificial Neural Network in python with a beautifully simple library called [Keras](http://keras.io)!\n",
    "\n",
    "So far in this series we have been implementing the low level functions ourselves but lets look at what some higher level libraries can do! Keras keeps all that crap in the background and lets us focus on crafting the actual architecture using pre made layers and training functions! \n",
    "\n",
    "Another advantage is that it can run ontop of both Tensorflow and Theano! I havent actually tested the Tensorflow backend however the theano backend work's great for running networks on your GPU! This give absolutely spectacular speed improvements over using the CPU alone.\n",
    "\n",
    "Check out how to use Theano with the GPU [here](http://deeplearning.net/software/theano/tutorial/using_gpu.html). I use the CUDA backend with CUDNN because I have an NVidia card, it also works spectacularly well!\n",
    "\n",
    "Ok, lets implement a simple ANN in Keras! We will be using another library called [kerosene](https://github.com/dribnet/kerosene) to allow us easy access to the [Iris](https://archive.ics.uci.edu/ml/datasets/Iris) dataset, you dont need to down load the data set, Kerosene will do that for us. Install kerosene using ```pip install kerosene```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "an artificial neural network to tackle the iris data set\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from kerosene.datasets import iris\n",
    "from keras.models import Sequential # import model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten # import core layers\n",
    "from keras.utils import np_utils # import helper funcs\n",
    "from keras.regularizers import WeightRegularizer, l2 # import l2 regularizer\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "\n",
    "batch_size = 3\n",
    "nb_classes = 3\n",
    "nb_epoch = 300\n",
    "\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), = iris.load_data()\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "_, input_size = X_train.shape\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(16, input_dim=input_size))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(12))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(16))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(9))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(8))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(6))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Dense(5))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "#model.add(SReLU())\n",
    "##model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Dense(4))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "#model.add(SReLU())\n",
    "##model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2, validation_split=0.2)\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model_iris.png', show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How easy is that! We just implemented an Artificial Neural Network, trained on our GPU achieving a high accuracy aswell!\n",
    "\n",
    "Let's have a shot at a Convolutional Neural Network tackling the good old MNIST data set! We dont need to use Kerosene for this one as its already included in the native Keras datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "a convolutional neural network to tackle the mnist data set\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from keras.datasets import mnist # mnist # import data set\n",
    "from keras.models import Sequential # import model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten # import core layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D # import convolutional layers\n",
    "from keras.utils import np_utils # import helper funcs\n",
    "from keras.regularizers import WeightRegularizer, l2 # import l2 regularizer\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "\n",
    "batch_size = 512\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# input image dimensions\n",
    "img_chan, img_rows, img_cols = 1, 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 25\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_chan, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_chan, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# noise input\n",
    "percent_noise = 0.1\n",
    "noise = (1.0/255) * percent_noise\n",
    "model.add(GaussianNoise(noise, input_shape=(img_chan, img_rows, img_cols)))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='valid'))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256))#, W_regularizer=WeightRegularizer(l1=0.001, l2=0.001)))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=2, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(model.summary())\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png', show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are a couple of new concepts here. Firstly can you see the input layer? its actually a Gaussian Noise layer. This will slightly distort the input signals to give us a variance on the dataset. This has the effect of mitigating over fitting by increasing the effective sample size!\n",
    "\n",
    "Secondly can you see the convolutional layers? we are using a Kernel size of 3x3 and 25 filters per convolutional layer.\n",
    "\n",
    "We then add a Maxpool layer to reduce the amount of space the data takes up with only a minimal effect on the actual information contained there in. This is the beauty of convolutional networks, we can take massive inputs and reduce them down to much smaller arrays, which are still very rich in information.\n",
    "\n",
    "After this we flatten the output and feed it into a standard Artificial Neural Network.\n",
    "\n",
    "Another interesting choice here is the use of S-shaped Rectified Linear Units. These are a special type of activation function that build upon the original idea of a Rectified Linear unit in the sense that they don't saturate like standard Sigmoid neurons.\n",
    "\n",
    "How about we try a dataset that consists of colour images? Let's build another netowork but this time use the cifar-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "a convolutional neural network to tackle the CIFAR-10 data set\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from keras.datasets import cifar10 # import data set\n",
    "from keras.models import Sequential # import model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten # import core layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D # import convolutional layers\n",
    "from keras.utils import np_utils # import helper funcs\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "\n",
    "batch_size = 256\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# input image dimensions\n",
    "img_chan, img_rows, img_cols = 3, 32, 32\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 18\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_chan, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_chan, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# noise input\n",
    "percent_noise = 0.1\n",
    "noise = (1.0/255) * percent_noise\n",
    "model.add(GaussianNoise(noise, input_shape=(img_chan, img_rows, img_cols)))\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(SReLU())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#99.51 0.0, 0.0\n",
    "model.add(Dense(1024))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(SReLU())\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=2, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(model.summary())\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png', show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So simple!\n",
    "\n",
    "Now you can see how easy it is to actually use keras, i highly recommend that you get acquainted with it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
