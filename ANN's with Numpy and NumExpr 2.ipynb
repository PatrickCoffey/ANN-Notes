{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at a library called numexpr. Take a look at the [github page](https://github.com/pydata/numexpr) for it. As you can see we can get significant speed improvments using this!\n",
    "\n",
    "Start by taking a look at my previous post about [Neural Networks in Python using numpy](http://blog.schlerp.net/2016/7/neural-networks-in-python-1-numpy/). \n",
    "\n",
    "Let's swap the functions in the file we wrote in the original post to use numexpr where ever we can without changing the structure too much and making it difficult to understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numexpr as ne\n",
    "\n",
    "class NENonlinear(object):\n",
    "    \"\"\"\n",
    "    Nonlinear\n",
    "    ---------\n",
    "    this is used to set up a non linear for a\n",
    "    network. The idea is you can instantiate it \n",
    "    and set what type of non linear function it \n",
    "    will be for that particular neaural network\n",
    "    \"\"\"\n",
    "    \n",
    "    _FUNC_TYPES = ('sigmoid',\n",
    "                   'softmax',\n",
    "                   'relu',\n",
    "                   'tanh',\n",
    "                   'softplus')\n",
    "    \n",
    "    def __init__(self, func_type='sigmoid'):\n",
    "        if func_type in self._FUNC_TYPES:\n",
    "            if func_type == self._FUNC_TYPES[0]:\n",
    "                # sigmoid\n",
    "                self._FUNCTION = self._FUNC_TYPES[0]\n",
    "            elif func_type == self._FUNC_TYPES[1]:\n",
    "                # softmax\n",
    "                self._FUNCTION = self._FUNC_TYPES[1]\n",
    "            elif func_type == self._FUNC_TYPES[2]:\n",
    "                # relu\n",
    "                self._FUNCTION = self._FUNC_TYPES[2]\n",
    "            elif func_type == self._FUNC_TYPES[3]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[3]\n",
    "            elif func_type == self._FUNC_TYPES[4]:\n",
    "                # tanh\n",
    "                self._FUNCTION = self._FUNC_TYPES[4]\n",
    "        else:\n",
    "            # default to sigmoid on invalid choice?\n",
    "            print(\"incorrect option `{}`\".format(func_type))\n",
    "            print(\"defaulting to sigmoid\")\n",
    "            self._init_sigmoid()\n",
    "    \n",
    "    def __call__(self, x, derivative=False):\n",
    "        ret = None\n",
    "        if self._FUNCTION == self._FUNC_TYPES[0]:\n",
    "            # sigmoid\n",
    "            if derivative:\n",
    "                ret = ne.evaluate('x*(1-x)')\n",
    "            else:\n",
    "                try:\n",
    "                    ret = ne.evaluate('1/(1+exp(-x))')\n",
    "                except RuntimeWarning:\n",
    "                    ret = 0.0\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[1]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = ne.evaluate('2*(exp(x)/sum(exp(x)))')\n",
    "            else:\n",
    "                # from: https://gist.github.com/stober/1946926\n",
    "                #e_x = np.exp(x - np.max(x))\n",
    "                #ret = e_x / e_x.sum()\n",
    "                # from: http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "                ret = ne.evaluate('exp(x)/sum(exp(x), axis=0)')\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[2]:\n",
    "            # relu\n",
    "            if derivative:\n",
    "                # from below + http://www.derivative-calculator.net/\n",
    "                ret = ne.evaluate('2*(abs(x))')\n",
    "            else:\n",
    "                ret = ne.evaluate('x*(abs(x))')\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # tanh\n",
    "            if derivative:\n",
    "                # from my own memory of calculus :P\n",
    "                ret = ne.evaluate('1.0-x**2')\n",
    "            else:\n",
    "                ret = ne.evaluate('tanh(x)')\n",
    "        elif self._FUNCTION == self._FUNC_TYPES[3]:\n",
    "            # softmax\n",
    "            if derivative:\n",
    "                # from wikipedia\n",
    "                ret = ne.evaluate('1.0/(1+exp(-x))')\n",
    "            else:\n",
    "                ret = ne.evaluate('log(1+exp(x))')\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how we have kept the structure very similar? numexpr is an exceptionally easy modification to add to most nonlinears!\n",
    "\n",
    "Now lets implement an N-layered Neural network with numexpr and numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NENNN(object):\n",
    "    \"\"\"N-layered neural network\"\"\"\n",
    "    def __init__(self, inputs, weights, outputs, alpha):\n",
    "        self.trained_loops = 0\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self._ALPHA = alpha\n",
    "        self._num_of_weights = len(weights)\n",
    "        self._LAYER_DEFS = {}\n",
    "        self.WEIGHT_DATA = {}\n",
    "        self.LAYER_FUNC = {}\n",
    "        self.LAYERS = {}\n",
    "        for i in range(self._num_of_weights):\n",
    "            #(in, out, nonlin)\n",
    "            self._LAYER_DEFS[i] = {'in': weights[i][0],\n",
    "                                   'out': weights[i][1],\n",
    "                                   'nonlin': weights[i][2]}\n",
    "        print(self._LAYER_DEFS)\n",
    "        self._init_layers()\n",
    "    \n",
    "    def _init_layers(self):\n",
    "        for i in range(self._num_of_weights):\n",
    "            _in = self._LAYER_DEFS[i]['in']\n",
    "            _out = self._LAYER_DEFS[i]['out']\n",
    "            _nonlin = self._LAYER_DEFS[i]['nonlin']\n",
    "            self.WEIGHT_DATA[i] = np.random.randn(_in, _out)\n",
    "            self.LAYER_FUNC[i] = _nonlin\n",
    "    \n",
    "    def reset(self):\n",
    "        self._init_layers()\n",
    "    \n",
    "    def _do_layer(self, prev_layer, next_layer, nonlin):\n",
    "        \"\"\"Does the actual calcs between layers :)\"\"\"\n",
    "        ret = nonlin(np.dot(prev_layer, next_layer))\n",
    "        return ret\n",
    "\n",
    "    def train(self, x, y, train_loops=100):\n",
    "        for j in range(train_loops):\n",
    "            # set up layers\n",
    "            prev_layer = x\n",
    "            prev_y = y\n",
    "            next_weight = None\n",
    "            l = 0\n",
    "            self.LAYERS[l] = x\n",
    "            for i in range(self._num_of_weights):\n",
    "                l += 1\n",
    "                next_weight = self.WEIGHT_DATA[i]\n",
    "                nonlin = self.LAYER_FUNC[i]\n",
    "                current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "                self.LAYERS[l] = current_layer\n",
    "                prev_layer = current_layer\n",
    "            last_layer = current_layer\n",
    "            #print(last_layer)\n",
    "            #\n",
    "            #layer2_error = y - layer2\n",
    "            #layer2_delta = layer2_error * self.non_lin(layer2, derivative=True)\n",
    "            \n",
    "            #layer1_error = layer2_delta.dot(self.w_out.T)\n",
    "            #layer1_delta = layer1_error * self.non_lin(layer1, derivative=True)\n",
    "            \n",
    "            #self.w_out += self._ALPHA * layer1.T.dot(layer2_delta)\n",
    "            #self.w_in += self._ALPHA * layer0.T.dot(layer1_delta)              \n",
    "            \n",
    "            # calculate errors\n",
    "            output_error = ne.evaluate('y - last_layer')\n",
    "            output_nonlin = self.LAYER_FUNC[self._num_of_weights - 1]\n",
    "            output_delta = output_error * output_nonlin(last_layer, derivative=True)\n",
    "\n",
    "            prev_delta = output_delta\n",
    "            prev_layer = last_layer\n",
    "            for i in reversed(range(self._num_of_weights)):\n",
    "                weight = self.WEIGHT_DATA[i]\n",
    "                current_weight_error = prev_delta.dot(weight.T)\n",
    "                current_weight_nonlin = self.LAYER_FUNC[i]\n",
    "                current_weight_delta = current_weight_error * current_weight_nonlin(self.LAYERS[i], derivative=True)\n",
    "                # backpropagate error\n",
    "                self.WEIGHT_DATA[i] += self._ALPHA * self.LAYERS[i].T.dot(prev_delta)\n",
    "                prev_delta = current_weight_delta\n",
    "                \n",
    "            # increment the train counter, so i can see how many \n",
    "            # loops my pickled nets have trained\n",
    "            self.trained_loops += 1\n",
    "            \n",
    "            # output important info\n",
    "            if (j % (train_loops/10)) == 0:\n",
    "                print(\"loop: {}\".format(j))\n",
    "                #print(\"Layer1 Error: {}\".format(np.mean(np.abs(layer1_error))))                \n",
    "                #print(\"Layer2 Error: {}\".format(np.mean(np.abs(layer2_error))))\n",
    "                print(\"Guess: \")\n",
    "                print(last_layer[0])\n",
    "                #print(\"output delta: \")\n",
    "                #print(np.round(output_delta, 2))\n",
    "                print(\"Guess (rounded): \")\n",
    "                print(np.round(last_layer[0], 1))\n",
    "                print(\"Actual: \")\n",
    "                print(y[0])\n",
    "        \n",
    "    def guess(self, x):\n",
    "        prev_layer = x\n",
    "        prev_y = y\n",
    "        next_weight = None\n",
    "        l = 0\n",
    "        self.LAYERS[l] = x\n",
    "        for i in range(self._num_of_weights):\n",
    "            l += 1\n",
    "            next_weight = self.WEIGHT_DATA[i]\n",
    "            nonlin = self.LAYER_FUNC[i]\n",
    "            current_layer = self._do_layer(prev_layer, next_weight, nonlin)\n",
    "            self.LAYERS[l] = current_layer\n",
    "            prev_layer = current_layer\n",
    "        last_layer = current_layer\n",
    "        return last_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
